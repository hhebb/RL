{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Critic\n",
    "---\n",
    "\n",
    "* PG 의 장점 + DQN 의 장점을 취함\n",
    "    * 두 방식의 결합이라고도 볼 수 있음\n",
    "    * 안정적이면서 빠름\n",
    "* PG 단점\n",
    "    * 가중치로 G_t 를 사용하면 더 좋은 action 의 의미가 퇴색될 가능성 있음\n",
    "    * M.C 기법으로 즉시 학습이 불가능하며 variance 가 높음\n",
    "    * DQN 의 부트스트랩 속성과 낮은 variance 속성을 합침\n",
    "* 수식을 통해 G_t 를 그냥 Q 또는 A 로 대체할 수 있음을 증명함\n",
    "* value based 알고리즘이 사실 Q 평가만 했기 때문에 critic 이라는 이름을 붙임\n",
    "* actor 네트워크, critic 네트워크 2 개 만듬\n",
    "    * 이 2 개를 완전히 분리할 수도 있고, 일부를 공유하게 만들 수도 있음\n",
    "* Advantage\n",
    "    * Q-V 값\n",
    "    * 각 action 이 평균에 비해 얼마나 좋은가?\n",
    "    * 덜 좋은 action 은 그냥 안좋은 것 (평균보다 안좋은 양수값을 음수로 만듬)\n",
    "    * 수치 자체가 적어지기 때문에 variance 도 줄어듬\n",
    "    * 그렇다고 V 네트워크를 또 분리하진 않음\n",
    "    * bellman optimality equation 을 쓰면 V 만으로 advantage 표현 가능\n",
    "    * 어떤 action 을 하던 간에 가장 좋은 상태가 되는 V 만 고려하게 됨\n",
    "    * 이제 이걸 Q 대신 log_pi 에 곱해주면 A2C 가 됨\n",
    "    * Q 네트워크 분리한다고 off policy 되지 않음\n",
    "    * critic 은 데이터 수집도, 액션 생성도 하지 않음\n",
    "    * 오로지 actor 가 둘 다 수행하고 그래서 on policy 임\n",
    "    * 근데 I.S 난 버퍼를 사용하면 off 가 됨 (ER, PER, HER)\n",
    "* Asynchronous\n",
    "    * 글로벌 A2C 네트워크를 비동기적으로 업데이트\n",
    "    * 여러 개의 로컬 네트워크가 병렬적, 독립적으로 상호작용\n",
    "    * exploration 에 유용하고 데이터 수집에 매우 유용함\n",
    "    * 여러 개의 환경이 동시에 돌고 그 결과를 글로벌 네트워크에 전달하기 때문에 어떻게 보면 리플레이 버퍼와 유사하다고 볼 수 있음\n",
    "    * 그래서 A3C 는 기본적으로 off policy\n",
    "    * 그러나 자기 자신이 과거에 경험한 것과 여러 로컬들이 경험한 것의 미묘한 차이가 있음\n",
    "    * 그러나 실제로 효과는 입증 불가? (A2C 도 충분히 좋다는 의견)\n",
    "* off policy 가 좋은 이유?\n",
    "    * exploration 의 문제임\n",
    "    * 상호작용 문제, POMDP 환경에서 탐험은 성능과 일반화에 거의 필수\n",
    "    * 또한 이미 수집된 데이터로 학습을 하므로 샘플 효율이 높음\n",
    "    * 샘플 효율은 off line 으로 가면서 더 높아짐\n",
    "    * 탐험은 결국 액션을 어떻게 선택하는가의 문제로 귀결됨\n",
    "    * sample 효율측면에서 가장 두드러짐\n",
    "* off policy 단점\n",
    "    * 그러나 과적합의 위험이 있음\n",
    "    * 또한 최적화하기 어려움 - 정책이 분리되어 있으니 분포 불일치라는 고질적인 문제\n",
    "* deadly triad\n",
    "    * 신경망 근사 + 부트스트랩 + off policy\n",
    "    * 최적화 문제를 극한의 어려움으로 만듬\n",
    "    * 주로 분포 불일치와 과도한 Q 과측정\n",
    "    * 그러나 이런 점들은 트릭으로 완화 가능\n",
    "    * 또 과측정은 유사한 state 에 방문했을 때 측정값이 그 근처 state 에서 덩달아 오르는 것이 일부 영향있음\n",
    "* RL 딜레마\n",
    "    * RL 은 지도학습과 다르게 답을 보고 배우는게 아니라 답을 찾는 학습방법임\n",
    "    * 그래서 탐색을 최대한 해야함\n",
    "    * 데이터가 이미지보다 엄청 많이 필요한데 그 데이터 수집이 이미지보다 더 어려움\n",
    "    * 그래서 학습 자체가 어렵고 학습하더라도 일반화가 안 됨\n",
    "    * 그러니까 다른 분야에서 해낸것 처럼 스케일업을 해야 함\n",
    "    * DRL 이 되면서 ML 모델들의 장점, 특징과 약점도 함께 받아들임\n",
    "* 편향 분산\n",
    "    * ML 에서 편향 분산과 다름\n",
    "    * RL 에서 편향이 크다는 것은 학습 한 후 실행할 때마다 잘못된 길로 계속 빠진다는 것\n",
    "    * RL 에서 분산이 크다는 것은 학습 한 후 실행할 때마다 결과가 중구난방이란 것\n",
    "    * 어쨌든 둘 다 일반화에 나쁜 상황임\n",
    "    * DRL 은 ML 모델의 편향 분산 문제까지 다 가져와버림\n",
    "    * 그러니까 DRL 은 더 문제가 많음\n",
    "    * 안그래도 어려운 RL 에 모델까지 붙이니까 더 다루기 어려워짐\n",
    "    * 일반화 달성과 차원의 저주를 벗어나기 위해서 모델을 도입했지만 그만큼 더 섬세한 제어가 필요\n",
    "    * 차원의 저주는 차원이 늘어날때마다 필요한 학습에 데이터는 기하급수적 증가의 문제를 의미\n",
    "* 편향 분산의 근본적 원인\n",
    "    * MC 방법은 에피소드 단위로 업데이트 하는데 그 에피소드 마다 내용이 제 각각임\n",
    "    * 그렇게 학습을 하니까 학습 이후에 돌릴 때마다 제각각임\n",
    "    * TD 방법은 부트스트랩 하면서 이전 상태와 격차 (TD error) 를 줄이려고 함\n",
    "    * 그러니까 일관성은 있는데 잘못된 길로 잘 빠질 수 있음\n",
    "    * 어쨌든 둘 다 데이터를 늘리면 누그러듬\n",
    "    * 대신 TD 방식이 스텝마다 학습하기 때문에 수렴이 더 빠름\n",
    "* discrete vs continuous\n",
    "    * 이 떡밥에 대해 너무 많이 작성했지만 또 차이점이 있음\n",
    "    * continouous 되는 순간, Q (critic) 는 더 이상 state 만 받아 action 별 값을 출력하지 못함\n",
    "    * 그래서 그 떄부터는 Q 는 state 와 action 을 모두 입력으로 받는 신경망이 되어야 함\n",
    "    * 물론 discrete action 에서도 위처럼 표현할 수 있지만 state 만 받는게 더 효율적임\n",
    "    * 물론 그 연속적 action 갯수는 여러 개가 될 수 있고 갯수 자체는 discrete 함\n",
    "    * 그리고 state, action 을 모두 받기 때문에 네트워크가 어느정도 복잡도가 커지는 걸 감수해야 할 듯\n",
    "* deterministic vs stochastic\n",
    "    * 이건 또 다른 대형 떡밥이었고 이것도 또 논할게 남아 있음\n",
    "    * 결정론적으로 변한 policy 는 action 의 확률성이 제거되어 버림\n",
    "    * 그래서 policy 는 확률 분포가 아니라 결정 값이 됨\n",
    "    * 대부분 이 땐 연속적 action 인 경우가 많음\n",
    "* actor 와 critic 이 대부분의 가중치를 공유하고 head 만 따로 뺄 수 있음\n",
    "    * 하지만 이 경우 불안정하게 될 가능성이 높음\n",
    "    * 약간의 업데이트라도 네트워크 전체에 영향을 주므로\n",
    "    * 간단한 환경에서만 사용하는게 좋을 듯\n",
    "* critic 도 여러가지 선택권이 있음\n",
    "    * continuous 이면 입력은 state, action 둘 다 줘야하지만 discrete 이면 state 만 입력으로 줘도 됨\n",
    "    * Q 가 아니라 A, V 를 출력한다면 action 에 대한 고려는 안 해도 됨\n",
    "    * 즉, 스칼라 값 하나만 출력하면 됨\n",
    "* DPG 에 대한 추가 정보\n",
    "    * action 에 대한 기댓값 계산이 빠지므로 더 적은 데이터로 잘 학습할 수 있는 data efficiency 가 높음\n",
    "    * stochastic PG 보다 성능이 좋음\n",
    "    * 기존의 방법들은 연속 action 에선 작동 안 됨\n",
    "* memo\n",
    "    * TD3 (twin delayed ddpg)\n",
    "    * ddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Cart Pole\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "                    help='random seed (default: 543)')\n",
    "parser.add_argument('--render', action='store_true',\n",
    "                    help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='interval between training status logs (default: 10)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "\n",
    "        # actor's layer\n",
    "        self.action_head = nn.Linear(128, 2)\n",
    "\n",
    "        # critic's layer\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        # action & reward buffer\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward of both actor and critic\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "\n",
    "        # critic: evaluates being in the state s_t\n",
    "        state_values = self.value_head(x)\n",
    "\n",
    "        # return values for both actor and critic as a tuple of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t\n",
    "        return action_prob, state_values\n",
    "\n",
    "\n",
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "\n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    m = Categorical(probs)\n",
    "\n",
    "    # and sample an action using the distribution\n",
    "    action = m.sample()\n",
    "\n",
    "    # save to action buffer\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "\n",
    "    # the action to take (left or right)\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    \"\"\"\n",
    "    Training code. Calculates actor and critic loss and performs backprop.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    value_losses = [] # list to save critic (value) loss\n",
    "    returns = [] # list to save the true values\n",
    "\n",
    "    # calculate the true value using rewards returned from the environment\n",
    "    for r in model.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + args.gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "\n",
    "        # calculate actor (policy) loss\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "        # calculate critic (value) loss using L1 smooth loss\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset rewards and action buffer\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "\n",
    "    # run infinitely many episodes\n",
    "    for i_episode in count(1):\n",
    "\n",
    "        # reset environment and episode reward\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        # for each episode, only run 9999 steps so that we don't\n",
    "        # infinite loop while learning\n",
    "        for t in range(1, 10000):\n",
    "\n",
    "            # select action from policy\n",
    "            action = select_action(state)\n",
    "\n",
    "            # take the action\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if args.render:\n",
    "                env.render()\n",
    "\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # update cumulative reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # perform backprop\n",
    "        finish_episode()\n",
    "\n",
    "        # log results\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "\n",
    "        # check if we have \"solved\" the cart pole problem\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
