{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPG\n",
    "---\n",
    "\n",
    "## natural gradient\n",
    "* 신경망 모델을 이루는 파라미터의 매니폴드가 업데이트 될 때\n",
    "* 기존에는 대부분 그냥 유클리드 공간에서 기울기를 구하고 경사하강을 적용하여 업데이트\n",
    "* 그런데 매니폴드 입장에서는 부당하다고 볼 수 있음\n",
    "* 매니폴드는 곡면이나 더 복잡한 형태로 형성되어 있을 수 있는데 그걸 무시하고 유클리드 공간에서 업데이트가 되어버리면 불안정하고 급격하게 바뀌는 결과 초래\n",
    "* 그래서 리만 공간에서 자연스러운 파라미터 업데이트하는 학습 갱신 방법 제시\n",
    "* natural 은 covariant\n",
    "* policy 를 이루는 파라미터들이 업데이트 될 때 매니폴드를 따라서 안정적으로 업데이트 하는 목적\n",
    "* RL 자체가 상호작용이 있어 민감하기 때문에, 다른 분야보다 안정적 업데이트에 더 힘을 써야함.\n",
    "* 결국 파라미터를 어떻게 진짜 steepest 한 방향으로 업데이트 할 것인가의 문제\n",
    "\n",
    "## 추가 설명\n",
    "* 원래 고차원 파라미터 공간에서 업데이트 하는 것이 기본\n",
    "* 그런데 특정 파라미터만이 최적화에 크리티컬한 영향을 미침\n",
    "* 이러한 파라미터들이 이루는 파라미터 매니폴드 라는 서브 스페이스를 생각할 수 있음\n",
    "* 이 공간을 리만 곡면, 리만 매니폴드라고 함\n",
    "* 이 곡면을 따라 파라미터의 경사 하강을 수행하면 더 중요한 파라미터 방향으로 업데이트가 되며 더 안정적이고 빠른 최적화가 가능함\n",
    "* 현재 파라미터 위치에서 국소적으로 리만 곡면을 추정하고 그 곡면을 따라 움직일 것\n",
    "* 근데 그 곡면은 양의 정부호 (일부 헤시안, FIM 등) 를 곱해서 구함\n",
    "* 그 행렬을 곱하면 자연스럽게 중요한 파라미터들로 이루어진 국소 곡면을 알 수 있음\n",
    "* 만약 그 행렬이 항등 행렬이면 곡면을 무시해버리고 일반적 경사하강처럼 모든 파라미터를 공평하게 영향을 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.utils import *\n",
    "from hparams import HyperParams as hp\n",
    "\n",
    "\n",
    "def get_returns(rewards, masks):\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    masks = torch.Tensor(masks)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "\n",
    "    running_returns = 0\n",
    "\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        running_returns = rewards[t] + hp.gamma * running_returns * masks[t]\n",
    "        returns[t] = running_returns\n",
    "\n",
    "    returns = (returns - returns.mean()) / returns.std()\n",
    "    return returns\n",
    "\n",
    "\n",
    "def get_loss(actor, returns, states, actions):\n",
    "    mu, std, logstd = actor(torch.Tensor(states))\n",
    "    log_policy = log_density(torch.Tensor(actions), mu, std, logstd)\n",
    "    returns = returns.unsqueeze(1)\n",
    "\n",
    "    objective = returns * log_policy\n",
    "    objective = objective.mean()\n",
    "    return objective\n",
    "\n",
    "\n",
    "def train_critic(critic, states, returns, critic_optim):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    n = len(states)\n",
    "    arr = np.arange(n)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        np.random.shuffle(arr)\n",
    "\n",
    "        for i in range(n // hp.batch_size):\n",
    "            batch_index = arr[hp.batch_size * i: hp.batch_size * (i + 1)]\n",
    "            batch_index = torch.LongTensor(batch_index)\n",
    "            inputs = torch.Tensor(states)[batch_index]\n",
    "            target = returns.unsqueeze(1)[batch_index]\n",
    "\n",
    "            values = critic(inputs)\n",
    "            loss = criterion(values, target)\n",
    "            critic_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            critic_optim.step()\n",
    "\n",
    "\n",
    "def fisher_vector_product(actor, states, p):\n",
    "    p.detach()\n",
    "    kl = kl_divergence(new_actor=actor, old_actor=actor, states=states)\n",
    "    kl = kl.mean()\n",
    "    kl_grad = torch.autograd.grad(kl, actor.parameters(), create_graph=True)\n",
    "    kl_grad = flat_grad(kl_grad)  # check kl_grad == 0\n",
    "\n",
    "    kl_grad_p = (kl_grad * p).sum()\n",
    "    kl_hessian_p = torch.autograd.grad(kl_grad_p, actor.parameters())\n",
    "    kl_hessian_p = flat_hessian(kl_hessian_p)\n",
    "\n",
    "    return kl_hessian_p + 0.1 * p\n",
    "\n",
    "\n",
    "# from openai baseline code\n",
    "# https://github.com/openai/baselines/blob/master/baselines/common/cg.py\n",
    "def conjugate_gradient(actor, states, b, nsteps, residual_tol=1e-10):\n",
    "    x = torch.zeros(b.size())\n",
    "    r = b.clone()\n",
    "    p = b.clone()\n",
    "    rdotr = torch.dot(r, r)\n",
    "    for i in range(nsteps):\n",
    "        _Avp = fisher_vector_product(actor, states, p)\n",
    "        alpha = rdotr / torch.dot(p, _Avp)\n",
    "        x += alpha * p\n",
    "        r -= alpha * _Avp\n",
    "        new_rdotr = torch.dot(r, r)\n",
    "        betta = new_rdotr / rdotr\n",
    "        p = r + betta * p\n",
    "        rdotr = new_rdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "\n",
    "def train_model(actor, critic, memory, actor_optim, critic_optim):\n",
    "    memory = np.array(memory)\n",
    "    states = np.vstack(memory[:, 0])\n",
    "    actions = list(memory[:, 1])\n",
    "    rewards = list(memory[:, 2])\n",
    "    masks = list(memory[:, 3])\n",
    "\n",
    "    # ----------------------------\n",
    "    # step 1: get returns\n",
    "    returns = get_returns(rewards, masks)\n",
    "\n",
    "    # ----------------------------\n",
    "    # step 2: train critic several steps with respect to returns\n",
    "    train_critic(critic, states, returns, critic_optim)\n",
    "\n",
    "    # ----------------------------\n",
    "    # step 3: get gradient of loss and hessian of kl\n",
    "    loss = get_loss(actor, returns, states, actions)\n",
    "    loss_grad = torch.autograd.grad(loss, actor.parameters())\n",
    "    loss_grad = flat_grad(loss_grad)\n",
    "    step_dir = conjugate_gradient(actor, states, loss_grad.data, nsteps=10)\n",
    "\n",
    "    # ----------------------------\n",
    "    # step 4: get step direction and step size and update actor\n",
    "    params = flat_params(actor)\n",
    "    new_params = params + 0.5 * step_dir\n",
    "    update_model(actor, new_params)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
