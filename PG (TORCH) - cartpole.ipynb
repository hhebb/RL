{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla PG\n",
    "---\n",
    "* value based 는 Q, V 를 직접적으로 계산하거나 근사했음\n",
    "    * 이 때 문제점은 TD error (q 러닝 계열) 가 항상 발생하면서 그게 계속 쌓인다는 점\n",
    "    * 너무 긍정적으로 평가하는데 비해 실제 reward 획득은 그에 훨씬 못 미치는 경우도 많음\n",
    "    * 그래서 점점 bias 가 되는 문제점\n",
    "    * policy 를 직접 학습하지 않고 Q 평가를 한다는 점에서 critic 모델이라고 볼 수 있음\n",
    "* 또 value based 는 discrete 액션만 다룰 수 있음\n",
    "    * 그리고 deterministic 한 policy 만을 만들어 냄\n",
    "* policy based\n",
    "    * p(s|a) 정책을 직접 계산하거나 근사함\n",
    "    * 즉, 가상의 Q 를 평가하지 않고 액션에 대한 결과를 직접 평가할 수 있음\n",
    "    * continous 액션 적용할 수 있고, stochastic policy 도 가능함\n",
    "    * 대신 variance 가 높아짐 - 샘플링 추정을 하는 RL 은 trade off 는 어쩔 수 없이 발생함\n",
    "    * variance - bias trade off 를 이해하고 총합을 줄이는 방법이 RL agent 를 안정화시키는 핵심 기술임\n",
    "    * 왜냐면 상호작용이라는 특성 때문에 환경이 조금만 달라져도 급격하게 성능이 무너지는 게 RL 이기 때문\n",
    "    * RL 을 다룰 땐 항상 근본적인 문제를 생각해야 함 - 불안정, 복잡성\n",
    "    * trajectory 를 보상 (R, Q, V, A 등) 에 따라 최적화하는 방식 - M.C 로 Q 학습하는 방식과 겉으로는 닮음\n",
    "    * 더 높은 보상, Q ,A 를 가지는 공간으로 trajectory (s, a, s, a) 를 구겨넣는 방식\n",
    "    * 여기서 trajectory 는 사실 신경망으로 표현되므로 사실상 traj 를 압축하는 w 를 고차원 공간상에서 업데이트 하는 거라고 생각해야함\n",
    "* RL 의 근본 개념\n",
    "    * 사실 RL 의 가장 근원적인 정의를 구현해낸 알고리즘.\n",
    "    * 아이러니하게 value based 가 먼저 나왔지만 Q 라는 추상적인 평가 함수 (기대 보상 합, reward to go) 로 기술해냄\n",
    "    * PG 는 보상의 최대화라는 목적을 충실하게 이행함\n",
    "* PG\n",
    "    * 일단, on policy 알고리즘\n",
    "    * episode 끝날 때까지 roll out 을 모두 모으고 끝나면 한 스텝에 대한 정보씩 차례로 싹 업데이트 함\n",
    "    * 신경망으로 gradient 를 타고 올라가려면, optimizer 가 인식할 수 있는 pseudo loss 가 필요함\n",
    "    * 그게 J 함수\n",
    "* contiuous action\n",
    "    * PG 는 policy 를 직접적으로 모델링하는 알고리즘이기 때문에, 그 어떤 형태의 action 도 유연하게 다룰 수 있음\n",
    "    * discrete 는 물론이고 continuous action 도 표현가능함\n",
    "    * 네트워크 출력을 softmax 로 표현하면 discrete action\n",
    "    * 출력을 어떤 분포 (가우시안) 의 파라미터 (평균, 분산) 으로 표현하게 설계하면 얼마든지 continuous action 표현 가능\n",
    "    * discrete 표현은 DQN 처럼 e-greedy 방식으로 최종 action 선택할 수 있음\n",
    "    * categorical action 과 혼용하지만 구분할 수도 있음\n",
    "    * 여러 개 action 중 k 개를 선택해서 실행하는 구조도 고려할 수 있음\n",
    "    * 그러나 policy network 의 출력에 softmax 를 통과시켜 확률분포 형태를 나타내지 않으면 deterministic policy 로 봐야함 - 출력값을 그대로 쓰니까\n",
    "    * continuous action, stochastic policy 는 분포의 매개변수를 출력하는데, 이 매개변수에 대해 그래디언트를 계산하게 됨. J 에는 그 값으로 샘플링한 값을 사용하고 업데이트\n",
    "    * continuous action, deterministic policy 는 출력값 자체를 그대로 적용하는데, 이 출력 제어값 자체의 그래디언트를 계산하고 업데이트함\n",
    "    * objective 는 log prob * Q(or R) 형태를 띄는데 두 항 간의 수치 균형을 이루는 것이 안정적인 학습에 중요함\n",
    "    * 잘못 학습하면 이상한 action 만 뽑게 학습할 수 있음\n",
    "* action vs policy\n",
    "    * action - discrete action vs continuous action\n",
    "    * policy - stochastic policy vs deterministic policy\n",
    "    * 알고리즘 디자인에 따라 2x2 조합이 모두 가능함\n",
    "* deterministic policy 는 POMDP 상황에서 특히 문제가 될 수 있음\n",
    "    * 결국 exploration 차원의 문제\n",
    "    * stochastic 은 근본적으로 exploration 에 열려 있음\n",
    "    * stochastic 이지만 deterministic 하게 변하는 경우도 많음\n",
    "    * 이 때는 표준 편차가 매우 작게 되도록 학습이 되어 뾰족한 분포에서 샘플링을 하게 됨\n",
    "    * 그런데 또 어떤 문제에서는 50:50 등으로 완전 stochastic 하게 되고 이 때는 꽤 큰 표준 편차를 가질 수도 있음\n",
    "    * 그래서 뭐라고 단언하기가 어렵고 그래서 RL 은 학습과 학습 결과 해석이 어렵다.\n",
    "    * 근데 어쨌든 stochastic 한 환경에서는 데이터가 훨씬 더 많이 필요하게 되는게 일반적임\n",
    "* DPG\n",
    "    * continuous action + deterministic policy 는 알고리즘 고안이 다소 다름\n",
    "    * 그러나 이는 stochastic policy 의 특수한 형태라는 것이 증명됨\n",
    "    * action 선택이 deterministic 하므로 action 에 대해 기댓값 계산이 빠지고 state 에 대한 기댓값 계산만 남음\n",
    "    * 기본적으로 신경망을 쓰게 되면 on-policy 가 되었을 때 target 이 계속 변하는 문제가 있음\n",
    "    * deterministic 이면 exploration 이 부족해짐\n",
    "    * 이런 문제들 때문에 off-policy 방법인 actor-critic 알고리즘을 도입하게 됨\n",
    "    * 그래도 강제로 exploration 하게 하는 방법 있는듯 - aciton 에 noise 를 추가하는 등\n",
    "* actor-critic DPG\n",
    "    * actor loss 는 Q 의 값에 대해서만 정의됨\n",
    "    * PG theorem 에서는 log_pi 는 확률적 값에 서만 정의되는데 deterministic action 은 확률이 없으므로 그냥 제거함\n",
    "    * 그리고 critic 에서 나온 Q 값만 가지고 actor 를 업데이트 함\n",
    "    * 그리고 다시 한 번 말하지만 Q 값은 Q 값이지, 확률 값이 아님 - 확률은 policy 임\n",
    "    * actor-critic 에서 critic Q 는 기존 value based 와 다른것 같은데 이건 AC 알고리즘 리뷰에서 자세히 다룰 것\n",
    "\n",
    "* Monte-Carlo 방식 업데이트이므로 variance 가 높다\n",
    "\n",
    "---\n",
    "* policy based - 거의 on policy\n",
    "* value based - 거의 off policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import variable\n",
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE = 1000\n",
    "EPSILON = 1\n",
    "RATE = .001\n",
    "GAMMA = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.data = []\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.outlayer = nn.Linear(64, 2)\n",
    "        self.optimizer = optim.Adam(self.parameters(), RATE)\n",
    "        \n",
    "    def pi(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.outlayer(x), dim=0)\n",
    "        return x\n",
    "    \n",
    "    def train(self):\n",
    "        discounted = 0\n",
    "        for r, log_p in self.data[::-1]:\n",
    "            discounted = r + GAMMA * discounted\n",
    "            # loss 정의\n",
    "            loss = - log_p * discounted\n",
    "            # 그래디언트 계산\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        self.data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.0\n",
      "21.0\n",
      "12.0\n",
      "14.0\n",
      "10.0\n",
      "8.0\n",
      "12.0\n",
      "19.0\n",
      "30.0\n",
      "21.0\n",
      "18.0\n",
      "62.0\n",
      "39.0\n",
      "27.0\n",
      "19.0\n",
      "23.0\n",
      "24.0\n",
      "16.0\n",
      "12.0\n",
      "17.0\n",
      "15.0\n",
      "20.0\n",
      "9.0\n",
      "41.0\n",
      "19.0\n",
      "18.0\n",
      "24.0\n",
      "10.0\n",
      "57.0\n",
      "11.0\n",
      "26.0\n",
      "44.0\n",
      "31.0\n",
      "16.0\n",
      "9.0\n",
      "26.0\n",
      "38.0\n",
      "19.0\n",
      "18.0\n",
      "19.0\n",
      "18.0\n",
      "27.0\n",
      "32.0\n",
      "25.0\n",
      "37.0\n",
      "23.0\n",
      "15.0\n",
      "39.0\n",
      "40.0\n",
      "20.0\n",
      "21.0\n",
      "20.0\n",
      "26.0\n",
      "33.0\n",
      "36.0\n",
      "45.0\n",
      "30.0\n",
      "18.0\n",
      "43.0\n",
      "13.0\n",
      "49.0\n",
      "23.0\n",
      "43.0\n",
      "26.0\n",
      "27.0\n",
      "25.0\n",
      "20.0\n",
      "28.0\n",
      "15.0\n",
      "9.0\n",
      "9.0\n",
      "25.0\n",
      "88.0\n",
      "19.0\n",
      "39.0\n",
      "51.0\n",
      "24.0\n",
      "49.0\n",
      "32.0\n",
      "20.0\n",
      "34.0\n",
      "21.0\n",
      "30.0\n",
      "66.0\n",
      "17.0\n",
      "40.0\n",
      "11.0\n",
      "23.0\n",
      "9.0\n",
      "28.0\n",
      "49.0\n",
      "40.0\n",
      "46.0\n",
      "26.0\n",
      "35.0\n",
      "20.0\n",
      "14.0\n",
      "32.0\n",
      "23.0\n",
      "31.0\n",
      "27.0\n",
      "72.0\n",
      "29.0\n",
      "99.0\n",
      "78.0\n",
      "40.0\n",
      "108.0\n",
      "88.0\n",
      "43.0\n",
      "57.0\n",
      "46.0\n",
      "126.0\n",
      "157.0\n",
      "70.0\n",
      "160.0\n",
      "132.0\n",
      "81.0\n",
      "56.0\n",
      "27.0\n",
      "90.0\n",
      "204.0\n",
      "106.0\n",
      "145.0\n",
      "151.0\n",
      "34.0\n",
      "103.0\n",
      "447.0\n",
      "146.0\n",
      "96.0\n",
      "156.0\n",
      "79.0\n",
      "49.0\n",
      "104.0\n",
      "75.0\n",
      "240.0\n",
      "551.0\n",
      "135.0\n",
      "112.0\n",
      "315.0\n",
      "676.0\n",
      "75.0\n",
      "1238.0\n",
      "137.0\n",
      "160.0\n",
      "521.0\n",
      "440.0\n",
      "404.0\n",
      "121.0\n",
      "593.0\n",
      "141.0\n",
      "217.0\n",
      "388.0\n",
      "300.0\n",
      "605.0\n",
      "570.0\n",
      "105.0\n",
      "133.0\n",
      "307.0\n",
      "101.0\n",
      "491.0\n",
      "127.0\n",
      "569.0\n",
      "294.0\n",
      "594.0\n",
      "158.0\n",
      "164.0\n",
      "640.0\n",
      "98.0\n",
      "302.0\n",
      "2000.0\n",
      "480.0\n",
      "447.0\n",
      "1796.0\n",
      "216.0\n",
      "241.0\n",
      "1179.0\n",
      "272.0\n",
      "186.0\n",
      "273.0\n",
      "276.0\n",
      "264.0\n",
      "203.0\n",
      "208.0\n",
      "398.0\n",
      "237.0\n",
      "306.0\n",
      "267.0\n",
      "275.0\n",
      "238.0\n",
      "328.0\n",
      "265.0\n",
      "398.0\n",
      "129.0\n",
      "199.0\n",
      "169.0\n",
      "194.0\n",
      "289.0\n",
      "192.0\n",
      "391.0\n",
      "238.0\n",
      "260.0\n",
      "198.0\n",
      "457.0\n",
      "198.0\n",
      "218.0\n",
      "379.0\n",
      "329.0\n",
      "193.0\n",
      "231.0\n",
      "209.0\n",
      "175.0\n",
      "223.0\n",
      "183.0\n",
      "290.0\n",
      "224.0\n",
      "319.0\n",
      "254.0\n",
      "207.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-736fab4c651f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;31m# 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-3932c41f7122>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;31m# 그래디언트 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "ep = 1\n",
    "net = Policy()\n",
    "\n",
    "while(ep < EPISODE):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while(not done):\n",
    "        #env.render()\n",
    "\n",
    "        # policy run\n",
    "        action_prob = net.pi(torch.from_numpy(state).float())\n",
    "        m = Categorical(action_prob)\n",
    "        a = m.sample()\n",
    "\n",
    "        # action 선택\n",
    "        if(EPSILON < random.randrange(0,1)):\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = a.item()\n",
    "\n",
    "        # step\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "\n",
    "        # reward 추가\n",
    "        total_reward += reward\n",
    "        \n",
    "        if(done):\n",
    "            reward = -100\n",
    "\n",
    "        # data 추가\n",
    "        net.data.append((reward, torch.log(action_prob[a])))\n",
    "\n",
    "        # state 갱신\n",
    "        state = state_next\n",
    "\n",
    "        # end episode\n",
    "        if(done):\n",
    "            print(total_reward)\n",
    "            total_reward = 0\n",
    "            ep += 1\n",
    "            \n",
    "            # env 초기화\n",
    "            state = env.reset()\n",
    "            EPSILON = 1 / (ep / 100 + 1)\n",
    "            \n",
    "            # 학습\n",
    "            net.train()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = nn.functional.relu(self.fc1(state))\n",
    "        a = nn.functional.relu(self.fc2(a))\n",
    "        return self.max_action * torch.tanh(self.fc3(a))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q = torch.cat([state, action], 1)\n",
    "        q = nn.functional.relu(self.fc1(q))\n",
    "        q = nn.functional.relu(self.fc2(q))\n",
    "        return self.fc3(q)\n",
    "\n",
    "class DPG:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=0.0001)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=0.001)\n",
    "        self.replay_buffer = deque(maxlen=1000000)\n",
    "        self.discount = 0.99\n",
    "        self.tau = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.expl_noise = 0.1\n",
    "        self.policy_noise = 0.2\n",
    "        self.noise_clip = 0.5\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1))\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        return action + np.random.normal(0, self.expl_noise, size=action.shape)\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state):\n",
    "        self.replay_buffer.append((state, action, reward, next_state))\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state = zip(*random.sample(self.replay_buffer, self.batch_size))\n",
    "        state = torch.FloatTensor(np.array(state))\n",
    "        action = torch.FloatTensor(np.array(action))\n",
    "        reward = torch.FloatTensor(np.array(reward)).reshape(-1, 1)\n",
    "        next_state = torch.FloatTensor(np.array(next_state))\n",
    "\n",
    "        target_q = reward + self.discount * self.critic(next_state, self.actor(next_state)).detach()\n",
    "        critic_loss = nn.MSELoss()(self.critic(state, action), target_q)\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def train_target(self):\n",
    "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def update_expl_noise(self, episode):\n",
    "        self.expl_noise *= 0.99\n",
    "\n",
    "    def update_policy_noise(self, episode):\n",
    "        if episode < 100:\n",
    "            self.policy_noise = 0.3\n",
    "        elif episode < 200:\n",
    "            self.policy_noise = 0.2\n",
    "        else:\n",
    "            self.policy_noise = 0.1\n",
    "\n",
    "    def update_noise_clip(self, episode):\n",
    "        if episode < 100:\n",
    "            self.noise_clip = 0.5\n",
    "        elif episode < 200:\n",
    "            self.noise_clip = 0.4\n",
    "        else:\n",
    "            self.noise_clip = 0.3\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        torch.save({\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'actor_optim_state_dict': self.actor_optim.state_dict(),\n",
    "            'critic_optim_state_dict': self.critic_optim.state_dict(),\n",
    "        }, filename)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "        self.actor_optim.load_state_dict(checkpoint['actor_optim_state_dict'])\n",
    "        self.critic_optim.load_state_dict(checkpoint['critic_optim_state_dict'])\n",
    "\n",
    "\n",
    "if __name__ == 'main':\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    state_dim = env.observation\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = env.action_space.high[0]\n",
    "    print(f'State dimension: {state_dim}, Action dimension: {action_dim}')\n",
    "\n",
    "    agent = DPG(state_dim, action_dim, max_action)\n",
    "\n",
    "    n_episodes = 100\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.store_experience(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            agent.train()\n",
    "\n",
    "        agent.update_expl_noise(i)\n",
    "        agent.update_policy_noise(i)\n",
    "        agent.update_noise_clip(i)\n",
    "        agent.train_target()\n",
    "\n",
    "        print(f'Episode {i}: Total reward = {episode_reward:.2f}')\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            agent.save_model(f'td3_model_episode_{i}.pt')\n",
    "\n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
