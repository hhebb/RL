{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRPO\n",
    "* trust region\n",
    "* 이론 배경 많이 필요\n",
    "* 이 알고리즘은 PG 와는 Policy Optimization 의 뿌리인데 전혀 다른 줄기로 새로운 알고리즘임\n",
    "    * 안정적으로 파라미터를 업데이트하는데에 집중하는 알고리즘들 중 하나\n",
    "    * NPG 처럼 안정적인 파라미터 업데이트 측면에 대한 연구\n",
    "* kakade 의 논문 - 출발점\n",
    "    * DP 에서 항상 성능이 보장되는 수식을 증명함\n",
    "    * 이 식은 old policy 로 현재 traj 를 평가할 수 있음도 포함됨\n",
    "    * 이 식은 time step 의 차원에서 정의된건데 관점을 바꿔 state 입장으로 바꿀 수 있음\n",
    "    * 그럴 때 state 에 대한 policy 를 old 로 바꿔야 계산에 수월해 짐\n",
    "    * 그게 가능한 이유는 old policy 근처에서는 둘 다 비슷하기 때문\n",
    "* conservative policy iteration\n",
    "    * 얼마나 비슷해야하는지는 잘 모름\n",
    "    * old, now policy 를 섞어쓰는 policy 를 제시함\n",
    "    * penalty 형태의 업데이트 식을 만듬\n",
    "    * 이 penalty 식을 따라 업데이트를 하면 항상 성능 향상이 보장되도록 업데이트 됨\n",
    "* general improvement\n",
    "    * mixed policy 가 아닌 일반적인 policy 에 적용해야함\n",
    "    * total variation divergence 를 이용\n",
    "    * old, new policy 의 거리를 계산하여 이용 - lower bound\n",
    "    * KLD 로 한 번 더 바꿈 - KL 이 TV^2 보다 항상 큼\n",
    "    * 제약조건이 두 policy 의 차이가 특정 값보다 작도록 함.\n",
    "    * 즉, 그냥 조금만 업데이트하도록 제한함\n",
    "* practical 한 알고리즘\n",
    "    * surrogate 함수를 최적화하는게 목적 함수를 최적화하는 것과 같음\n",
    "    * 저 대리 함수가 계속 다루던 페널티 함수\n",
    "    * penalty 함수를 보면 상수 C 가 너무 큰 값이라 업데이트가 실질적으로 불가능한 형태\n",
    "    * penalty 식을 제약조건 식으로 풀어 헤침\n",
    "    * 근데 이건 식 형태 자체를 바꾸는 거라서 엄밀성을 포기해버리는것임\n",
    "    * 제약조건만 만족하면 목적함수를 최대한으로 업데이트 할 것임\n",
    "    * 시그마로 표현된 식을 기댓값 형태로 바꿔서 샘플링이 가능하도록 해야함\n",
    "    * I.S 등이 사용됨\n",
    "* 최적화\n",
    "    * 이렇게 제약조건 최적화 문제로 만들면 이제 최적화 문제를 푸는 line search 등을 쓰면 됨\n",
    "    * conjugate gradient 를 위한 FIM 행렬 등도 쓰임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def conjugate_gradients(Avp, b, nsteps, residual_tol=1e-10):\n",
    "    x = torch.zeros(b.size())\n",
    "    r = b.clone()\n",
    "    p = b.clone()\n",
    "    rdotr = torch.dot(r, r)\n",
    "    for i in range(nsteps):\n",
    "        _Avp = Avp(p)\n",
    "        alpha = rdotr / torch.dot(p, _Avp)\n",
    "        x += alpha * p\n",
    "        r -= alpha * _Avp\n",
    "        new_rdotr = torch.dot(r, r)\n",
    "        betta = new_rdotr / rdotr\n",
    "        p = r + betta * p\n",
    "        rdotr = new_rdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "\n",
    "def linesearch(model,\n",
    "               f,\n",
    "               x,\n",
    "               fullstep,\n",
    "               expected_improve_rate,\n",
    "               max_backtracks=10,\n",
    "               accept_ratio=.1):\n",
    "    fval = f(True).data\n",
    "    print(\"fval before\", fval.item())\n",
    "    for (_n_backtracks, stepfrac) in enumerate(.5**np.arange(max_backtracks)):\n",
    "        xnew = x + stepfrac * fullstep\n",
    "        set_flat_params_to(model, xnew)\n",
    "        newfval = f(True).data\n",
    "        actual_improve = fval - newfval\n",
    "        expected_improve = expected_improve_rate * stepfrac\n",
    "        ratio = actual_improve / expected_improve\n",
    "        print(\"a/e/r\", actual_improve.item(), expected_improve.item(), ratio.item())\n",
    "\n",
    "        if ratio.item() > accept_ratio and actual_improve.item() > 0:\n",
    "            print(\"fval after\", newfval.item())\n",
    "            return True, xnew\n",
    "    return False, x\n",
    "\n",
    "\n",
    "def trpo_step(model, get_loss, get_kl, max_kl, damping):\n",
    "    loss = get_loss()\n",
    "    grads = torch.autograd.grad(loss, model.parameters())\n",
    "    loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n",
    "\n",
    "    def Fvp(v):\n",
    "        kl = get_kl()\n",
    "        kl = kl.mean()\n",
    "\n",
    "        grads = torch.autograd.grad(kl, model.parameters(), create_graph=True)\n",
    "        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
    "\n",
    "        kl_v = (flat_grad_kl * Variable(v)).sum()\n",
    "        grads = torch.autograd.grad(kl_v, model.parameters())\n",
    "        flat_grad_grad_kl = torch.cat([grad.contiguous().view(-1) for grad in grads]).data\n",
    "\n",
    "        return flat_grad_grad_kl + v * damping\n",
    "\n",
    "    stepdir = conjugate_gradients(Fvp, -loss_grad, 10)\n",
    "\n",
    "    shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n",
    "\n",
    "    lm = torch.sqrt(shs / max_kl)\n",
    "    fullstep = stepdir / lm[0]\n",
    "\n",
    "    neggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)\n",
    "    print((\"lagrange multiplier:\", lm[0], \"grad_norm:\", loss_grad.norm()))\n",
    "\n",
    "    prev_params = get_flat_params_from(model)\n",
    "    success, new_params = linesearch(model, get_loss, prev_params, fullstep,\n",
    "                                     neggdotstepdir / lm[0])\n",
    "    set_flat_params_to(model, new_params)\n",
    "\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
