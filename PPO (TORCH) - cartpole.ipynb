{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "---\n",
    "![title](https://spinningup.openai.com/en/latest/_images/math/0a399dc49e3b45664a7edaf485ab5c23a7282f43.svg)\n",
    "---\n",
    "\n",
    "* on-policy 알고리즘 - episode 중간에 policy 업데이트\n",
    "* GAE (Generalized Advantage Estimate) 사용\n",
    "* T step 만큼 움직인 후 실시간 업데이트\n",
    "* continuous, discrete action 모두 활용가능\n",
    "\n",
    "* first order 방법\n",
    "* 굉장히 간단하고 실용적이며 성능도 쓸만함. 하지만 최고성능은 아님\n",
    "* 이산, 연속 액션 공간 모두 사용 가능\n",
    "    * deterministic policy 를 쓰려면 변형이 필요\n",
    "* 그런데 TRPO 와 동일한 문제 인식에서 출발하고 해결 방식도 유사성이 있음\n",
    "    * NPG 와도 같은 문제 인식임.\n",
    "    * 파라미터 업데이트 과정에 집중함\n",
    "    * 원랜 모델 파라미터 업데이트할 때 스텝 사이즈를 학습률로 수동으로 지정함\n",
    "    * 근데 TRPO, PPO 등은 그 크기를 안정적인 업데이트를 보장하는 범위내로 적응형으로 자동으로 조절하는 개념\n",
    "    * TRPO, PPO 는 A.C 와는 완전히 다른 업데이트 식 (업데이트 방식도) 을 가지는 다른 알고리즘임을 인지해야함\n",
    "* 일종의 트릭을 쓰는데 매우 단순\n",
    "    * TRPO 는 제약조건 있는 최적화 문제를 풀기 위해 2 차 최적화 기법 사용\n",
    "    * PPO 는 그냥 간단하게 clip\n",
    "* 그냥 TRPO 의 목적함수에 policy 비율이 있는데 그냥 이 항의 값이 일정 크기 이내로 들어오게만 제한 걸어버림\n",
    "    * 어쨌든 TRPO 처럼 monotonic 성능 향상을 보장\n",
    "* kld 등을 써서 제약을 거는 이유가 policy 많이 변하지 말라고 쓰는건데 목적함수에서 그걸 제약을 걸어버릴 수 있음\n",
    "* 다른 sota 들도 있는데 구현이 간단하고 파라미터 튜닝 요소가 적어 여전히 많이 쓰임\n",
    "* 샘플마다 한 번 학습하는게 아니라 mini batch 를 여러 번 학습 함\n",
    "* 스텝 샘플링과 surrogate 함수 optimization 를 번갈아가며 수행\n",
    "    * 여러 대리 함수 중 clipping 이 가장 성능 좋음\n",
    "* 대부분 전통적인 PG 와 같이 on-policy 임\n",
    "    * 행동 생성 policy 와 학습 policy 가 같음\n",
    "    * 얘가 off-policy 되려면 버퍼를 만드는 수 밖에 없음\n",
    "    * DQN 은 애초에 e-greedy, max policy 차이 때문에 off-policy 임\n",
    "    * on-policy 이기 때문에 기본적으로 모든 샘플을 다음 회차 때 폐기함\n",
    "    * on-policy 이기 때문에 탐험이 제한되고 로컬 미니멈에 갇힐 수도 있음\n",
    "    * 과적합은 off-policy 에서 더 위험\n",
    "* 실제로 최종 목적함수에는 exploration 을 위한 엔트로피 항도 추가\n",
    "    * 사실 Q 함수의 항도 들어가서 총 3 개를 학습함\n",
    "* GAE\n",
    "    * 많은 PG 알고리즘들이 GAE 를 사용해서 안정적인 업데이트\n",
    "\n",
    "---\n",
    "## torch 신경망 주의할 것\n",
    "* 업데이트 할 파라미터 정확히 지정하기 - detach 로 학습할 파라미터 확실하게 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "ALPHA = .001\n",
    "EPSILON = 1\n",
    "T = 20 # T step 만큼 데이터 쌓고 학습할 것\n",
    "LAMBDA = .95\n",
    "K = 3\n",
    "GAMMA = .99\n",
    "e = .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc_pi = nn.Linear(256, 2)\n",
    "        self.fc_v = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), ALPHA)\n",
    "    \n",
    "    def pi(self, x, softmax_dim=0):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        prob = torch.softmax(self.fc_pi(x), dim = softmax_dim) # batch 처리 (학습할 떈 1 차원)\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc_v(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, data, optimizer):\n",
    "    s, a, r, s2, d, prob = batch_factory(data)\n",
    "\n",
    "    # epoch K 만큼\n",
    "    for i in range(K):\n",
    "        td_target = r + GAMMA * net.v(s2)\n",
    "        delta = td_target - net.v(s)\n",
    "        delta = delta.detach().numpy() # 1 step advantage\n",
    "        advantage_lst = []\n",
    "        advantage = 0.0\n",
    "\n",
    "        # GAE 계산\n",
    "        for delta_t in delta[::-1]:\n",
    "            advantage = GAMMA * LAMBDA * advantage + delta_t[0]\n",
    "            advantage_lst.append([advantage])\n",
    "        advantage_lst.reverse()\n",
    "        advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "        pi = net.pi(s, softmax_dim=1)\n",
    "        pi_a = pi.gather(1, a)\n",
    "        ratio = torch.exp(torch.log(pi_a) - torch.log(prob))\n",
    "\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - e, 1 + e) * advantage\n",
    "        loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(net.v(s) , td_target.detach())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_factory(memory):\n",
    "    s_, a_, r_, s2_, d_, prob_ = [], [], [], [], [], []\n",
    "    for s, a, r, s2, d, p in memory:\n",
    "        s_.append(s)\n",
    "        a_.append([a])\n",
    "        r = -100 if d else r\n",
    "        r_.append([r])\n",
    "        s2_.append(s2)\n",
    "        d = 0 if d else 1\n",
    "        d_.append([d])\n",
    "        prob_.append([p])\n",
    "        \n",
    "    s_ = torch.tensor(s_, dtype=torch.float)\n",
    "    a_ = torch.tensor(a_)\n",
    "    r_ = torch.tensor(r_, dtype=torch.float)\n",
    "    s2_ = torch.tensor(s2_, dtype=torch.float)\n",
    "    d_ = torch.tensor(d_, dtype=torch.float)\n",
    "    prob_ = torch.tensor(prob_)\n",
    "    \n",
    "    return s_, a_, r_, s2_, d_, prob_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20.6\n",
      "20 24.8\n",
      "30 39.5\n",
      "40 21.8\n",
      "50 40.9\n",
      "60 43.5\n",
      "70 53.0\n",
      "80 64.7\n",
      "90 72.3\n",
      "100 76.2\n",
      "110 86.3\n",
      "120 127.8\n",
      "130 66.1\n",
      "140 85.6\n",
      "150 128.2\n",
      "160 219.4\n",
      "170 147.1\n",
      "180 231.1\n",
      "190 248.9\n",
      "200 187.8\n",
      "210 534.2\n",
      "220 397.8\n",
      "230 173.2\n",
      "240 223.2\n",
      "250 305.4\n",
      "260 147.8\n",
      "270 212.4\n",
      "280 92.2\n",
      "290 143.3\n",
      "300 204.0\n",
      "310 171.8\n",
      "320 48.8\n",
      "330 24.0\n",
      "340 23.0\n",
      "350 25.8\n",
      "360 123.2\n",
      "370 132.0\n",
      "380 185.9\n",
      "390 122.1\n",
      "400 174.8\n",
      "410 186.7\n",
      "420 137.8\n",
      "430 112.5\n",
      "440 230.0\n",
      "450 420.6\n",
      "460 269.9\n",
      "470 195.0\n",
      "480 596.1\n",
      "490 1374.5\n",
      "500 497.0\n",
      "510 523.8\n",
      "520 1053.8\n",
      "530 108.9\n",
      "540 212.4\n",
      "550 1097.7\n",
      "560 538.2\n",
      "570 874.5\n",
      "580 1787.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-8b7fd3790e0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mtrain_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-151-d6f19df91dac>\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(net, data, optimizer)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = PPO()\n",
    "ep = 1\n",
    "total_ep = 10000\n",
    "gamma = .95\n",
    "total_reward = 0\n",
    "data = []\n",
    "optimizer = optim.Adam(net.parameters(), ALPHA)\n",
    "\n",
    "while(ep < total_ep):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while(not done):\n",
    "        # T step 움직인 후 clipping - T 가 너무 크면 불안정??\n",
    "        for t in range(T):\n",
    "            prob = net.pi(torch.from_numpy(state).float())\n",
    "            action = Categorical(prob).sample().item()\n",
    "            state_next, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            data.append((state, action, reward/100.0, state_next, done, prob[action].item()))\n",
    "            state = state_next\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        train_net(net, data, optimizer)\n",
    "        data = []\n",
    "        \n",
    "    ep += 1\n",
    "    if(ep%10 == 0):\n",
    "        print(ep, total_reward/10.0)\n",
    "        total_reward = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
